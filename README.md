# UncertaintyPolicy
The project Uncertainty-Aware Hallucination Detection and Clarifying-Question Policy for LLMs tests whether uncertainty signals in LLM outputs can drive a useful policy to decide between answering, clarifying and abstaining. This project was part of the course Natural Language Processing at Seoul National University in Fall 2025
